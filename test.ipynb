{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7921e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced text splitter with metadata extraction for scientific papers.\n",
    "Supports boundary-aware chunking with production-grade citation/equation handling.\n",
    "Features:\n",
    "  - Robust citation detection (numeric, author-year, LaTeX \\\\cite{})\n",
    "  - Multi-line LaTeX equation support\n",
    "  - Chronological boundary detection (preserves text order)\n",
    "  - Smart merging of small blocks for coherence\n",
    "  - Rich metadata extraction for reranking\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Optional, Dict, Tuple, NamedTuple\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata associated with a text chunk\"\"\"\n",
    "    section: str  # Main section (Abstract, Introduction, etc.)\n",
    "    subsection: str  # Subsection if available\n",
    "    chunk_index: int  # Index within the section\n",
    "    hierarchy_level: int  # 0 for main, 1 for subsection, etc.\n",
    "    content_type: str  # \"text\", \"equation\", \"figure\", \"table\", \"reference\"\n",
    "    has_citations: bool  # Whether chunk contains citations\n",
    "    citation_count: int  # Number of citations\n",
    "    has_equations: bool  # Whether chunk contains equations\n",
    "    equation_count: int  # Number of equations\n",
    "    is_abstract: bool  # Whether chunk is from abstract (typically more important)\n",
    "    is_conclusion: bool  # Whether chunk is from conclusion\n",
    "    importance_score: float  # 0.0-1.0 score based on content type and position\n",
    "\n",
    "class BoundaryMatch(NamedTuple):\n",
    "    \"\"\"Represents a boundary match with its position\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    pattern_type: str  # Type of boundary (e.g., \"heading\", \"equation\", \"paragraph\")\n",
    "\n",
    "class BoundaryAwareTextSplitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 512,\n",
    "        chunk_overlap: int = 50,\n",
    "        boundary_patterns: Optional[List[Tuple[str, str]]] = None,\n",
    "        min_chunk_size: int = 100,\n",
    "        merge_small_chunks: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize boundary-aware text splitter.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Target chunk size in characters\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            boundary_patterns: List of (pattern, pattern_type) tuples\n",
    "            min_chunk_size: Minimum size before merging\n",
    "            merge_small_chunks: Whether to merge small chunks for coherence\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.merge_small_chunks = merge_small_chunks\n",
    "        \n",
    "        # Boundary patterns with types for better chronological ordering\n",
    "        self.boundary_patterns = boundary_patterns or [\n",
    "            # Priority 1: Section headers (must be handled first)\n",
    "            (r\"^#{1,6}\\s+(.+)$\", \"heading_markdown\"),  # Markdown headers\n",
    "            (r\"^(\\d+(?:\\.\\d+)*)\\s+([A-Z].+?)(?:\\n|$)\", \"heading_numbered\"),  # Numbered sections\n",
    "            \n",
    "            # Priority 2: Major section boundaries (with case-insensitive for common variations)\n",
    "            (r\"^(Abstract|Introduction|Background|Methodology|Method|Results|\"\n",
    "             r\"Discussion|Conclusion|Acknowledgments?|References|Appendix)\\b\", \"section_boundary\"),\n",
    "            (r\"^(?:related\\s+works?|related\\s+work)\\b\", \"section_boundary\", re.IGNORECASE),  # Related Work variations\n",
    "            \n",
    "            # Priority 3: Equations (multi-line support)\n",
    "            (r\"\\\\begin\\{(?:equation|align|align\\*|gather|gather\\*|multline|displaymath|equation\\*)\\}.+?\"\n",
    "             r\"\\\\end\\{(?:equation|align|align\\*|gather|gather\\*|multline|displaymath|equation\\*)\\}\", \"equation_block\"),\n",
    "            (r\"\\$\\$.+?\\$\\$\", \"equation_inline_display\"),\n",
    "            (r\"\\\\\\[.+?\\\\\\]\", \"equation_latex_brackets\"),\n",
    "            \n",
    "            # Priority 4: Tables and figures (CAPTION patterns must flush chunk)\n",
    "            (r\"^\\s*(?:Table|Figure)\\s+\\d+(?:\\.\\d+)?:.*$\", \"caption\"),\n",
    "            (r\"^\\s*(?:Table|Figure)\\s*\\d+\", \"table_figure\"),\n",
    "            \n",
    "            # Priority 5: Captions and centered content\n",
    "            (r\"<center>.+?</center>\", \"centered_text\"),\n",
    "            \n",
    "            # Priority 6: Lists (numbered and bulleted)\n",
    "            (r\"^[\\s]*[\\d]+\\.\\s+\", \"numbered_list\"),\n",
    "            (r\"^[\\s]*[-•*]\\s+\", \"bullet_list\"),\n",
    "            \n",
    "            # Priority 7: Paragraph boundaries\n",
    "            (r\"\\n\\n+\", \"paragraph_break\"),\n",
    "            \n",
    "            # Priority 8: Citations (must be last to not interfere with others)\n",
    "            (r\"\\\\\\cite\\{[^}]+\\}\", \"cite_latex\"),\n",
    "            (r\"\\[(?:\\d+(?:,\\s*\\d+)*|\\d+\\s*[–-]\\s*\\d+)\\]\", \"cite_numeric\"),\n",
    "            (r\"\\((?:[A-Za-z\\s]+(?:et\\s+al\\.)?),?\\s*\\d{4}[a-z]?\\)\", \"cite_author_year\"),\n",
    "            (r\"[A-Z][a-z]+\\s+(?:et\\s+al\\.)?,\\s*\\d{4}\", \"cite_author_year_inline\"),\n",
    "        ]\n",
    "        \n",
    "        # Initialize LangChain's RecursiveCharacterTextSplitter as backup\n",
    "        self.backup_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def _find_all_boundaries(self, text: str) -> List[BoundaryMatch]:\n",
    "        \"\"\"\n",
    "        Find ALL boundaries in chronological order (by position in text).\n",
    "        This is crucial for maintaining text order and avoiding duplicate splits.\n",
    "        Handles regex flags properly for case-insensitive patterns.\n",
    "        \n",
    "        Returns:\n",
    "            List of BoundaryMatch sorted by start position\n",
    "        \"\"\"\n",
    "        all_matches = []\n",
    "        \n",
    "        # Use DOTALL flag to handle multi-line patterns\n",
    "        for pattern_tuple in self.boundary_patterns:\n",
    "            # Handle both 2-tuple and 3-tuple formats (pattern, type) or (pattern, type, flags)\n",
    "            if len(pattern_tuple) == 3:\n",
    "                pattern, pattern_type, flags = pattern_tuple\n",
    "                base_flags = re.MULTILINE | re.DOTALL | flags\n",
    "            else:\n",
    "                pattern, pattern_type = pattern_tuple\n",
    "                base_flags = re.MULTILINE | re.DOTALL\n",
    "            \n",
    "            try:\n",
    "                for match in re.finditer(pattern, text, base_flags):\n",
    "                    # Avoid duplicates at same position\n",
    "                    if not any(m.start == match.start() and m.end == match.end() for m in all_matches):\n",
    "                        all_matches.append(BoundaryMatch(\n",
    "                            start=match.start(),\n",
    "                            end=match.end(),\n",
    "                            pattern_type=pattern_type\n",
    "                        ))\n",
    "            except re.error:\n",
    "                # Silently ignore regex errors if a pattern is bad\n",
    "                continue\n",
    "        \n",
    "        # Sort by start position (chronological order)\n",
    "        all_matches.sort(key=lambda x: (x.start, -x.end))  # Sort by start, then longer matches first\n",
    "        \n",
    "        # Remove overlapping boundaries (keep the first/longest one)\n",
    "        filtered_matches = []\n",
    "        for match in all_matches:\n",
    "            # Check if this boundary overlaps with an already-added boundary\n",
    "            overlaps = any(\n",
    "                (fm.start <= match.start < fm.end) or (fm.start < match.end <= fm.end)\n",
    "                for fm in filtered_matches\n",
    "            )\n",
    "            if not overlaps:\n",
    "                filtered_matches.append(match)\n",
    "        \n",
    "        return filtered_matches\n",
    "\n",
    "    def _count_citations_advanced(self, chunk: str) -> int:\n",
    "        \"\"\"\n",
    "        Count citations with support for multiple citation formats.\n",
    "        Handles: [1], [1,2], [1-3], [1–3], (Smith et al., 2023), Smith (2022), \\\\cite{...}\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        \n",
    "        # Numeric citations: [1], [1, 2], [1–3], [1-3]\n",
    "        numeric_citations = re.findall(\n",
    "            r\"\\[(?:\\d+(?:[,\\s]+\\d+)*|(?:\\d+\\s*[–-]\\s*\\d+))\\]\",\n",
    "            chunk\n",
    "        )\n",
    "        count += len(numeric_citations)\n",
    "        \n",
    "        # Author-year citations: (Smith et al., 2023), (Smith, 2023)\n",
    "        author_year = re.findall(\n",
    "            r\"\\([A-Za-z\\s]+(?:et\\s+al\\.)?[,\\s]*\\d{4}[a-z]?\\)\",\n",
    "            chunk\n",
    "        )\n",
    "        count += len(author_year)\n",
    "        \n",
    "        # Author-year inline: Smith et al. (2023), Smith (2022)\n",
    "        author_inline = re.findall(\n",
    "            r\"[A-Z][a-z]+(?:\\s+et\\s+al\\.)?[,\\s]*\\(\\d{4}[a-z]?\\)\",\n",
    "            chunk\n",
    "        )\n",
    "        count += len(author_inline)\n",
    "        \n",
    "        # LaTeX citations: \\cite{...}\n",
    "        latex_cites = re.findall(r\"\\\\\\cite\\{[^}]+\\}\", chunk)\n",
    "        count += len(latex_cites)\n",
    "        \n",
    "        return count\n",
    "\n",
    "    def _count_equations_advanced(self, chunk: str) -> int:\n",
    "        \"\"\"\n",
    "        Count equations with support for multi-line LaTeX environments.\n",
    "        Handles nested structures and various equation environments.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        \n",
    "        # Display equations with $$\n",
    "        count += len(re.findall(r\"\\$\\$\", chunk)) // 2\n",
    "        \n",
    "        # LaTeX bracket equations \\\\[...\\\\]\n",
    "        count += len(re.findall(r\"\\\\\\[.*?\\\\\\]\", chunk, re.DOTALL))\n",
    "        \n",
    "        # Equation environments (align, gather, multline, etc.)\n",
    "        count += len(re.findall(\n",
    "            r\"\\\\begin\\{(?:equation|align|align\\*|gather|gather\\*|multline|displaymath|equation\\*)\\}\",\n",
    "            chunk\n",
    "        ))\n",
    "        \n",
    "        # Inline math with single $\n",
    "        inline_math = re.findall(r\"(?<!\\$)\\$(?!\\$)[^$]+\\$(?!\\$)\", chunk)\n",
    "        count += len(inline_math)\n",
    "        \n",
    "        return count # Đã xóa logger\n",
    "\n",
    "    def _extract_section_info(self, text: str, boundary_pattern_type: Optional[str] = None) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Extract section name and hierarchy level from text.\n",
    "        Prioritizes boundary pattern type to avoid mis-detection in multi-line content.\n",
    "        \n",
    "        Args:\n",
    "            text: The text chunk to analyze\n",
    "            boundary_pattern_type: The pattern type from boundary detection (if available)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (section_name, hierarchy_level)\n",
    "        \"\"\"\n",
    "        # Priority 1: Use boundary pattern type if available (most reliable)\n",
    "        if boundary_pattern_type == \"section_boundary\":\n",
    "            # Extract section name from text\n",
    "            match = re.search(\n",
    "                r\"^(?:related\\s+works?|related\\s+work|abstract|introduction|background|methodology|method|\"\n",
    "                r\"results|discussion|conclusion|acknowledgments?|references|appendix)\\b\",\n",
    "                text,\n",
    "                re.MULTILINE | re.IGNORECASE\n",
    "            )\n",
    "            if match:\n",
    "                return match.group(0).title(), 1\n",
    "        \n",
    "        # Priority 2: Check for markdown headers\n",
    "        headers = {\n",
    "            r\"^#\\s+(.+)$\": 1,\n",
    "            r\"^##\\s+(.+)$\": 2,\n",
    "            r\"^###\\s+(.+)$\": 3,\n",
    "            r\"^####\\s+(.+)$\": 4,\n",
    "            r\"^#####\\s+(.+)$\": 5,\n",
    "            r\"^######\\s+(.+)$\": 6,\n",
    "        }\n",
    "        \n",
    "        for pattern, level in headers.items():\n",
    "            match = re.search(pattern, text, re.MULTILINE)\n",
    "            if match:\n",
    "                return match.group(1).strip(), level\n",
    "        \n",
    "        # Priority 3: Check for numbered sections\n",
    "        numbered_match = re.search(r\"^(\\d+(?:\\.\\d+)*)\\s+(.+)$\", text, re.MULTILINE)\n",
    "        if numbered_match:\n",
    "            section_name = numbered_match.group(2).strip()\n",
    "            level = len(numbered_match.group(1).split('.'))\n",
    "            return section_name, level\n",
    "        \n",
    "        return \"General\", 0\n",
    "\n",
    "    def _identify_content_type(self, chunk: str) -> str:\n",
    "        \"\"\"Identify the type of content in the chunk\"\"\"\n",
    "        if re.search(r\"\\$\\$|\\\\begin\\{|\\\\end\\{|\\\\\\[|\\\\\\]\", chunk):\n",
    "            return \"equation\"\n",
    "        elif re.search(r\"^(Table|Figure)\\s+\\d+\", chunk, re.MULTILINE):\n",
    "            return \"table\" if chunk.startswith(\"Table\") else \"figure\"\n",
    "        elif re.search(r\"^\\[\\d+\\]\\s+\", chunk, re.MULTILINE):\n",
    "            return \"reference\"\n",
    "        elif re.search(r\"^(Abstract|Introduction|Methodology|Results|Discussion|Conclusion)\", chunk):\n",
    "            return \"section_header\"\n",
    "        else:\n",
    "            return \"text\"\n",
    "\n",
    "    def _extract_metadata(\n",
    "        self,\n",
    "        chunk: str,\n",
    "        chunk_index: int,\n",
    "        current_section: str,\n",
    "        boundary_pattern_type: Optional[str] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract comprehensive metadata for a chunk.\n",
    "        \n",
    "        Args:\n",
    "            chunk: The text chunk\n",
    "            chunk_index: Index of chunk\n",
    "            current_section: Current section name\n",
    "            boundary_pattern_type: Type of boundary pattern detected (for accurate section detection)\n",
    "        \"\"\"\n",
    "        # Count citations (advanced)\n",
    "        citation_count = self._count_citations_advanced(chunk)\n",
    "        \n",
    "        # Count equations (advanced)\n",
    "        equation_count = self._count_equations_advanced(chunk)\n",
    "        \n",
    "        # Determine content type\n",
    "        content_type = self._identify_content_type(chunk)\n",
    "        \n",
    "        # Extract section info - pass boundary type for accurate detection\n",
    "        section_name, hierarchy_level = self._extract_section_info(chunk, boundary_pattern_type)\n",
    "        \n",
    "        # Determine if abstract or conclusion\n",
    "        is_abstract = \"abstract\" in section_name.lower()\n",
    "        is_conclusion = \"conclusion\" in section_name.lower()\n",
    "        \n",
    "        # Calculate importance score\n",
    "        importance_score = self._calculate_importance_score(\n",
    "            content_type=content_type,\n",
    "            hierarchy_level=hierarchy_level,\n",
    "            is_abstract=is_abstract,\n",
    "            is_conclusion=is_conclusion,\n",
    "            citation_count=citation_count\n",
    "        )\n",
    "        \n",
    "        # Use detected section or keep current section\n",
    "        if section_name != \"General\":\n",
    "            current_section = section_name\n",
    "        \n",
    "        metadata = {\n",
    "            \"section\": current_section,\n",
    "            \"subsection\": section_name if section_name != \"General\" else \"\",\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"hierarchy_level\": hierarchy_level,\n",
    "            \"content_type\": content_type,\n",
    "            \"has_citations\": citation_count > 0,\n",
    "            \"citation_count\": citation_count,\n",
    "            \"has_equations\": equation_count > 0,\n",
    "            \"equation_count\": equation_count,\n",
    "            \"is_abstract\": is_abstract,\n",
    "            \"is_conclusion\": is_conclusion,\n",
    "            \"importance_score\": importance_score,\n",
    "            \"chunk_length\": len(chunk),\n",
    "            \"word_count\": len(chunk.split()),\n",
    "            \"boundary_pattern_type\": boundary_pattern_type or \"unknown\"\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def _calculate_importance_score(\n",
    "        self,\n",
    "        content_type: str,\n",
    "        hierarchy_level: int,\n",
    "        is_abstract: bool,\n",
    "        is_conclusion: bool,\n",
    "        citation_count: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate importance score for prioritizing chunks\"\"\"\n",
    "        score = 0.5  # Base score\n",
    "        \n",
    "        # Content type impact\n",
    "        content_weights = {\n",
    "            \"abstract\": 1.0,\n",
    "            \"conclusion\": 0.9,\n",
    "            \"equation\": 0.8,\n",
    "            \"section_header\": 0.7,\n",
    "            \"reference\": 0.4,\n",
    "            \"table\": 0.6,\n",
    "            \"figure\": 0.6,\n",
    "            \"text\": 0.5\n",
    "        }\n",
    "        score += content_weights.get(content_type, 0.5) * 0.3\n",
    "        \n",
    "        # Hierarchy level impact (lower level = higher importance)\n",
    "        if hierarchy_level > 0:\n",
    "            score += max(0, (4 - hierarchy_level) / 4) * 0.2\n",
    "        \n",
    "        # Section importance\n",
    "        if is_abstract:\n",
    "            score += 0.3\n",
    "        if is_conclusion:\n",
    "            score += 0.25\n",
    "        \n",
    "        # Citation impact (more citations = more important)\n",
    "        citation_bonus = min(citation_count / 5, 0.5) * 0.2\n",
    "        score += citation_bonus\n",
    "        \n",
    "        return min(1.0, score)\n",
    "\n",
    "    def split_text(self, text: str) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"\n",
    "        Split text using chronologically-ordered boundary detection with metadata.\n",
    "        Handles multi-line equations, various citation formats, and maintains text coherence.\n",
    "        Smart flush for captions to keep them separate from regular text.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (chunk_text, metadata)\n",
    "        \"\"\"\n",
    "        # Find all boundaries in chronological order\n",
    "        boundaries = self._find_all_boundaries(text)\n",
    "        \n",
    "        if not boundaries:\n",
    "            # No boundaries found, treat entire text as one chunk\n",
    "            metadata = self._extract_metadata(text.strip(), 0, \"General\")\n",
    "            return [(text.strip(), metadata)]\n",
    "        \n",
    "        chunks_raw = []\n",
    "        last_pos = 0\n",
    "        current_section = \"Introduction\"\n",
    "        \n",
    "        # Split text at boundaries in chronological order\n",
    "        for boundary in boundaries:\n",
    "            # Add text before boundary\n",
    "            chunk_before = text[last_pos:boundary.start].strip()\n",
    "            if chunk_before:\n",
    "                chunks_raw.append((chunk_before, \"text\"))\n",
    "            \n",
    "            # Add the boundary itself\n",
    "            boundary_text = text[boundary.start:boundary.end].strip()\n",
    "            if boundary_text:\n",
    "                chunks_raw.append((boundary_text, boundary.pattern_type))\n",
    "            \n",
    "            last_pos = boundary.end\n",
    "        \n",
    "        # Add remaining text after last boundary\n",
    "        if last_pos < len(text):\n",
    "            remaining = text[last_pos:].strip()\n",
    "            if remaining:\n",
    "                chunks_raw.append((remaining, \"text\"))\n",
    "        \n",
    "        # Merge and filter chunks with smart caption flushing\n",
    "        chunks_with_metadata = []\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_type = \"text\"\n",
    "        chunk_index = 0\n",
    "        \n",
    "        for i, (chunk_text, pattern_type) in enumerate(chunks_raw):\n",
    "            # Smart caption handling: if current chunk has content and we encounter a caption, flush immediately\n",
    "            is_caption = pattern_type == \"caption\"\n",
    "            should_flush_for_caption = is_caption and current_chunk and len(current_chunk) > self.min_chunk_size\n",
    "            \n",
    "            # Try to add to current chunk\n",
    "            if not should_flush_for_caption and len(current_chunk) + len(chunk_text) <= self.chunk_size:\n",
    "                if current_chunk:\n",
    "                    current_chunk += \"\\n\" + chunk_text\n",
    "                else:\n",
    "                    current_chunk = chunk_text\n",
    "                    current_chunk_type = pattern_type\n",
    "            else:\n",
    "                # Current chunk would exceed size or caption encountered, save it and start new one\n",
    "                if current_chunk and len(current_chunk) > self.min_chunk_size:\n",
    "                    metadata = self._extract_metadata(\n",
    "                        current_chunk,\n",
    "                        chunk_index,\n",
    "                        current_section,\n",
    "                        current_chunk_type\n",
    "                    )\n",
    "                    chunks_with_metadata.append((current_chunk, metadata))\n",
    "                    if metadata[\"subsection\"]:\n",
    "                        current_section = metadata[\"subsection\"]\n",
    "                    chunk_index += 1\n",
    "                \n",
    "                current_chunk = chunk_text\n",
    "                current_chunk_type = pattern_type\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk and len(current_chunk) > self.min_chunk_size:\n",
    "            metadata = self._extract_metadata(\n",
    "                current_chunk,\n",
    "                chunk_index,\n",
    "                current_section,\n",
    "                current_chunk_type\n",
    "            )\n",
    "            chunks_with_metadata.append((current_chunk, metadata))\n",
    "        elif current_chunk:\n",
    "            # Small chunk at end - try to merge with previous\n",
    "            if chunks_with_metadata:\n",
    "                prev_chunk, prev_metadata = chunks_with_metadata[-1]\n",
    "                merged_chunk = prev_chunk + \"\\n\" + current_chunk\n",
    "                if len(merged_chunk) <= self.chunk_size * 1.2:  # Allow slight overflow for merging\n",
    "                    merged_metadata = self._extract_metadata(\n",
    "                        merged_chunk,\n",
    "                        prev_metadata[\"chunk_index\"],\n",
    "                        prev_metadata[\"section\"],\n",
    "                        prev_metadata.get(\"boundary_pattern_type\", \"text\")\n",
    "                    )\n",
    "                    chunks_with_metadata[-1] = (merged_chunk, merged_metadata)\n",
    "                else:\n",
    "                    metadata = self._extract_metadata(\n",
    "                        current_chunk,\n",
    "                        chunk_index,\n",
    "                        current_section,\n",
    "                        current_chunk_type\n",
    "                    )\n",
    "                    chunks_with_metadata.append((current_chunk, metadata))\n",
    "        \n",
    "        # Final pass: Split chunks that are still too large\n",
    "        final_chunks = []\n",
    "        for chunk, metadata in chunks_with_metadata:\n",
    "            if len(chunk) > self.chunk_size:\n",
    "                # Use backup splitter\n",
    "                backup_chunks = self.backup_splitter.split_text(chunk)\n",
    "                for i, sub_chunk in enumerate(backup_chunks):\n",
    "                    new_metadata = metadata.copy()\n",
    "                    new_metadata[\"chunk_index\"] = len(final_chunks)\n",
    "                    final_chunks.append((sub_chunk, new_metadata))\n",
    "            else:\n",
    "                final_chunks.append((chunk, metadata))\n",
    "        \n",
    "        return final_chunks\n",
    "\n",
    "    def split_documents(self, documents: List[str]) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"\n",
    "        Split multiple documents while maintaining document boundaries.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (chunk_text, metadata)\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = self.split_text(doc)\n",
    "            all_chunks.extend(chunks)\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf439d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEmbedder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        collection_name: str = \"scientific_papers\",\n",
    "        dim: int = 1024,\n",
    "        milvus_host: str = \"localhost\",\n",
    "        milvus_port: int = 19530,\n",
    "        chunk_size: int = 512,\n",
    "        chunk_overlap: int = 50\n",
    "    ):\n",
    "        # Initialize the embedding model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.dim = dim\n",
    "\n",
    "        # Initialize text splitter\n",
    "        self.text_splitter = BoundaryAwareTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        # Connect to Milvus\n",
    "        connections.connect(host=milvus_host, port=milvus_port)\n",
    "\n",
    "        # Create collection if it doesn't exist\n",
    "        self.collection_name = collection_name\n",
    "        if not utility.has_collection(collection_name):\n",
    "            self._create_collection(dim)\n",
    "\n",
    "        self.collection = Collection(collection_name)\n",
    "        self.collection.load()\n",
    "\n",
    "    def _create_collection(self, dim: int):\n",
    "        \"\"\"Create a new Milvus collection with rich metadata schema.\"\"\"\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "            FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n",
    "            # Metadata fields for better reranking\n",
    "            FieldSchema(name=\"section\", dtype=DataType.VARCHAR, max_length=256),\n",
    "            FieldSchema(name=\"subsection\", dtype=DataType.VARCHAR, max_length=256),\n",
    "            FieldSchema(name=\"content_type\", dtype=DataType.VARCHAR, max_length=50),\n",
    "            FieldSchema(name=\"hierarchy_level\", dtype=DataType.INT8),\n",
    "            FieldSchema(name=\"importance_score\", dtype=DataType.FLOAT),\n",
    "            FieldSchema(name=\"citation_count\", dtype=DataType.INT32),\n",
    "            FieldSchema(name=\"equation_count\", dtype=DataType.INT32),\n",
    "            FieldSchema(name=\"is_abstract\", dtype=DataType.BOOL),\n",
    "            FieldSchema(name=\"is_conclusion\", dtype=DataType.BOOL),\n",
    "            FieldSchema(name=\"word_count\", dtype=DataType.INT32),\n",
    "            FieldSchema(name=\"chunk_index\", dtype=DataType.INT32),\n",
    "            # Source metadata\n",
    "            FieldSchema(name=\"source_file\", dtype=DataType.VARCHAR, max_length=512),\n",
    "            FieldSchema(name=\"processing_timestamp\", dtype=DataType.VARCHAR, max_length=30),\n",
    "        ]\n",
    "        schema = CollectionSchema(\n",
    "            fields=fields,\n",
    "            description=\"Scientific paper chunks collection with rich metadata\"\n",
    "        )\n",
    "        Collection(self.collection_name, schema)\n",
    "\n",
    "    def _get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of text chunks.\"\"\"\n",
    "        encoded_input = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            embeddings = model_output.last_hidden_state[:, 0]\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "    def read_scientific_paper(self, file_path: str) -> str:\n",
    "        \"\"\"Read and preprocess the scientific paper from final_text.txt\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "\n",
    "    def process_document(self, text: str, metadata: Optional[Dict] = None, batch_size: int = 32):\n",
    "        \"\"\"Process a document by splitting it with metadata and generating embeddings.\"\"\"\n",
    "        # Split text into chunks with metadata\n",
    "        chunks_with_metadata = self.text_splitter.split_text(text)\n",
    "\n",
    "        # Add document-level metadata\n",
    "        doc_metadata = metadata or {}\n",
    "        source_file = doc_metadata.get(\"source_file\", \"unknown\")\n",
    "        processing_timestamp = doc_metadata.get(\"processing_timestamp\", datetime.datetime.now().isoformat())\n",
    "\n",
    "        # Process chunks in batches\n",
    "        for i in range(0, len(chunks_with_metadata), batch_size):\n",
    "            batch = chunks_with_metadata[i:i + batch_size]\n",
    "            batch_chunks = [item[0] for item in batch]\n",
    "            batch_chunk_metadata = [item[1] for item in batch]\n",
    "\n",
    "            batch_embeddings = self._get_embeddings(batch_chunks)\n",
    "\n",
    "            # Insert into Milvus with full metadata\n",
    "            entities = []\n",
    "            for chunk, chunk_meta, embedding in zip(\n",
    "                batch_chunks,\n",
    "                batch_chunk_metadata,\n",
    "                batch_embeddings\n",
    "            ):\n",
    "                entity = {\n",
    "                    \"text\": chunk,\n",
    "                    \"embeddings\": embedding.tolist(),\n",
    "                    # Chunk-level metadata\n",
    "                    \"section\": chunk_meta.get(\"section\", \"unknown\"),\n",
    "                    \"subsection\": chunk_meta.get(\"subsection\", \"\"),\n",
    "                    \"content_type\": chunk_meta.get(\"content_type\", \"text\"),\n",
    "                    \"hierarchy_level\": chunk_meta.get(\"hierarchy_level\", 0),\n",
    "                    \"importance_score\": chunk_meta.get(\"importance_score\", 0.5),\n",
    "                    \"citation_count\": chunk_meta.get(\"citation_count\", 0),\n",
    "                    \"equation_count\": chunk_meta.get(\"equation_count\", 0),\n",
    "                    \"is_abstract\": chunk_meta.get(\"is_abstract\", False),\n",
    "                    \"is_conclusion\": chunk_meta.get(\"is_conclusion\", False),\n",
    "                    \"word_count\": chunk_meta.get(\"word_count\", 0),\n",
    "                    \"chunk_index\": chunk_meta.get(\"chunk_index\", i),\n",
    "                    # Source metadata\n",
    "                    \"source_file\": source_file,\n",
    "                    \"processing_timestamp\": processing_timestamp,\n",
    "                }\n",
    "                entities.append(entity)\n",
    "\n",
    "            self.collection.insert(entities)\n",
    "\n",
    "        # Flush to ensure data is written\n",
    "        self.collection.flush()\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        score_threshold: float = 0.5\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"Search for similar chunks using the query.\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self._get_embeddings([query])[0]\n",
    "\n",
    "        # Search in Milvus\n",
    "        search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}\n",
    "        results = self.collection.search(\n",
    "            data=[query_embedding.tolist()],\n",
    "            anns_field=\"embeddings\",\n",
    "            param=search_params,\n",
    "            limit=top_k,\n",
    "            output_fields=[\"text\", \"section\", \"source_file\"] # Thêm output fields cho kết quả\n",
    "        )\n",
    "\n",
    "        # Format results\n",
    "        matches = []\n",
    "        for hits in results:\n",
    "            for hit in hits:\n",
    "                if hit.score >= score_threshold:\n",
    "                    matches.append({\n",
    "                        \"text\": hit.entity.get(\"text\"),\n",
    "                        \"score\": hit.score,\n",
    "                        \"section\": hit.entity.get(\"section\", \"N/A\"),\n",
    "                        \"source_file\": hit.entity.get(\"source_file\", \"N/A\"),\n",
    "                    })\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def process_scientific_paper(self, file_path: str, paper_metadata: Optional[Dict] = None):\n",
    "        \"\"\"Process a scientific paper from file and store in vector database\"\"\"\n",
    "        content = self.read_scientific_paper(file_path)\n",
    "\n",
    "        if paper_metadata is None:\n",
    "            paper_metadata = {\n",
    "                \"source_file\": file_path,\n",
    "                \"paper_type\": \"scientific_paper\",\n",
    "                \"processing_timestamp\": datetime.datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "        self.process_document(content, metadata=paper_metadata)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up connections.\"\"\"\n",
    "        self.collection.release()\n",
    "        connections.disconnect(\"default\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Hàm main để xử lý các bài báo khoa học và tạo embeddings.\n",
    "    (Sử dụng các tham số được gán cứng thay vì đối số dòng lệnh)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Các tham số gán cứng ---\n",
    "    data_dir_str = \"../../data/processed\"\n",
    "    file_name = \"final_text.txt\"\n",
    "    collection_name = \"scientific_papers\"\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    milvus_host = \"localhost\"\n",
    "    milvus_port = 19530\n",
    "    chunk_size = 512\n",
    "    chunk_overlap = 50\n",
    "    \n",
    "    # Đặt thành True nếu bạn muốn xóa và tạo lại collection\n",
    "    force_recreate = False \n",
    "    \n",
    "    # Đặt một câu truy vấn để kiểm tra, hoặc đặt là None để bỏ qua\n",
    "    query = \"What is the main challenge of this research?\" \n",
    "    query_top_k = 5\n",
    "    # ----------------------------\n",
    "\n",
    "    # Giải quyết đường dẫn file\n",
    "    script_dir = Path(__file__).parent.absolute()\n",
    "    data_dir = script_dir.parent.parent / data_dir_str\n",
    "    file_path = data_dir / file_name\n",
    "\n",
    "    # Kiểm tra file tồn tại\n",
    "    if not file_path.exists():\n",
    "        print(f\"Lỗi: Không tìm thấy file: {file_path}\")\n",
    "        if data_dir.exists():\n",
    "            print(\"Các file có sẵn trong thư mục dữ liệu:\")\n",
    "            for file in data_dir.glob(\"*\"):\n",
    "                print(f\"  - {file.name}\")\n",
    "        sys.exit(1) # Thoát nếu không tìm thấy file\n",
    "\n",
    "    try:\n",
    "        # Tái tạo collection nếu được yêu cầu\n",
    "        if force_recreate:\n",
    "            print(f\"Đang xóa collection '{collection_name}' (nếu tồn tại)...\")\n",
    "            connections.connect(\n",
    "                host=milvus_host,\n",
    "                port=milvus_port\n",
    "            )\n",
    "            if utility.has_collection(collection_name):\n",
    "                utility.drop_collection(collection_name)\n",
    "                print(\"Đã xóa collection cũ.\")\n",
    "            connections.disconnect(\"default\")\n",
    "\n",
    "        # Khởi tạo DocumentEmbedder\n",
    "        print(\"Đang khởi tạo DocumentEmbedder...\")\n",
    "        embedder = DocumentEmbedder(\n",
    "            model_name=model_name,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            collection_name=collection_name,\n",
    "            dim=1024,  # BGE-large có dim 1024\n",
    "            milvus_host=milvus_host,\n",
    "            milvus_port=milvus_port,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        print(\"Khởi tạo thành công.\")\n",
    "\n",
    "        # Chuẩn bị metadata cho tài liệu\n",
    "        paper_metadata = {\n",
    "            \"source_file\": str(file_path),\n",
    "            \"paper_type\": \"scientific_paper\",\n",
    "            \"processing_timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"file_size_mb\": file_path.stat().st_size / 1024 / 1024\n",
    "        }\n",
    "\n",
    "        # Xử lý bài báo khoa học\n",
    "        print(f\"Đang xử lý file: {file_path}...\")\n",
    "        embedder.process_scientific_paper(str(file_path), paper_metadata)\n",
    "        print(\"Hoàn tất xử lý và nạp dữ liệu vào Milvus.\")\n",
    "\n",
    "        # Kiểm tra bằng truy vấn nếu có\n",
    "        if query:\n",
    "            print(f\"\\nĐang thực hiện truy vấn kiểm tra: '{query}'\")\n",
    "            results = embedder.search(\n",
    "                query=query,\n",
    "                top_k=query_top_k,\n",
    "                score_threshold=0.0 # Lấy tất cả kết quả để xem\n",
    "            )\n",
    "\n",
    "            if results:\n",
    "                print(\"\\n--- Kết quả tìm kiếm ---\")\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"[{i}] Điểm tương đồng (Score): {result['score']:.4f}\")\n",
    "                    print(f\"    Nguồn: {result['source_file'].split(os.path.sep)[-1]}\")\n",
    "                    print(f\"    Phần (Section): {result['section']}\")\n",
    "                    preview = result['text'][:200].replace('\\n', ' ')\n",
    "                    print(f\"    Nội dung: {preview}...\")\n",
    "                    print(\"-\" * 25)\n",
    "            else:\n",
    "                print(\"Không tìm thấy kết quả nào cho truy vấn.\")\n",
    "\n",
    "        # Đóng kết nối\n",
    "        print(\"\\nĐang đóng kết nối...\")\n",
    "        embedder.close()\n",
    "        print(\"Hoàn tất.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình thực thi: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f54fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang khởi tạo DocumentEmbedder...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Hàm main để xử lý các bài báo khoa học và tạo embeddings.\n",
    "    (Sử dụng các tham số được gán cứng thay vì đối số dòng lệnh)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Các tham số gán cứng ---\n",
    "    data_dir_str = \"../../data/processed\"\n",
    "    file_name = \"final_text.txt\"\n",
    "    collection_name = \"scientific_papers\"\n",
    "    model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "    milvus_host = \"localhost\"\n",
    "    milvus_port = 19530\n",
    "    chunk_size = 512\n",
    "    chunk_overlap = 50\n",
    "    file_path = data_dir_str + file_name\n",
    "    \n",
    "    # Đặt thành True nếu bạn muốn xóa và tạo lại collection\n",
    "    force_recreate = False \n",
    "    \n",
    "    # Đặt một câu truy vấn để kiểm tra, hoặc đặt là None để bỏ qua\n",
    "    query = \"What is the main challenge of this research?\" \n",
    "    query_top_k = 5\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        # Tái tạo collection nếu được yêu cầu\n",
    "        if force_recreate:\n",
    "            print(f\"Đang xóa collection '{collection_name}' (nếu tồn tại)...\")\n",
    "            connections.connect(\n",
    "                host=milvus_host,\n",
    "                port=milvus_port\n",
    "            )\n",
    "            if utility.has_collection(collection_name):\n",
    "                utility.drop_collection(collection_name)\n",
    "                print(\"Đã xóa collection cũ.\")\n",
    "            connections.disconnect(\"default\")\n",
    "\n",
    "        # Khởi tạo DocumentEmbedder\n",
    "        print(\"Đang khởi tạo DocumentEmbedder...\")\n",
    "        embedder = DocumentEmbedder(\n",
    "            model_name=model_name,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            collection_name=collection_name,\n",
    "            dim=1024,  # BGE-large có dim 1024\n",
    "            milvus_host=milvus_host,\n",
    "            milvus_port=milvus_port,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        print(\"Khởi tạo thành công.\")\n",
    "\n",
    "        # Chuẩn bị metadata cho tài liệu\n",
    "        paper_metadata = {\n",
    "            \"source_file\": str(file_path),\n",
    "            \"paper_type\": \"scientific_paper\",\n",
    "            \"processing_timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"file_size_mb\": file_path.stat().st_size / 1024 / 1024\n",
    "        }\n",
    "\n",
    "        # Xử lý bài báo khoa học\n",
    "        print(f\"Đang xử lý file: {file_path}...\")\n",
    "        embedder.process_scientific_paper(str(file_path), paper_metadata)\n",
    "        print(\"Hoàn tất xử lý và nạp dữ liệu vào Milvus.\")\n",
    "\n",
    "        # Kiểm tra bằng truy vấn nếu có\n",
    "        if query:\n",
    "            print(f\"\\nĐang thực hiện truy vấn kiểm tra: '{query}'\")\n",
    "            results = embedder.search(\n",
    "                query=query,\n",
    "                top_k=query_top_k,\n",
    "                score_threshold=0.0 # Lấy tất cả kết quả để xem\n",
    "            )\n",
    "\n",
    "            if results:\n",
    "                print(\"\\n--- Kết quả tìm kiếm ---\")\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"[{i}] Điểm tương đồng (Score): {result['score']:.4f}\")\n",
    "                    print(f\"    Nguồn: {result['source_file'].split(os.path.sep)[-1]}\")\n",
    "                    print(f\"    Phần (Section): {result['section']}\")\n",
    "                    preview = result['text'][:200].replace('\\n', ' ')\n",
    "                    print(f\"    Nội dung: {preview}...\")\n",
    "                    print(\"-\" * 25)\n",
    "            else:\n",
    "                print(\"Không tìm thấy kết quả nào cho truy vấn.\")\n",
    "\n",
    "        # Đóng kết nối\n",
    "        print(\"\\nĐang đóng kết nối...\")\n",
    "        embedder.close()\n",
    "        print(\"Hoàn tất.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình thực thi: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "from pymilvus import connections, utility\n",
    "    \n",
    "# (Lớp DocumentEmbedder phải được định nghĩa ở trên)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
