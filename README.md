# ðŸš€ RAG Pipeline - Complete Implementation Summary

## Project Overview

A fully functional **Retrieval-Augmented Generation (RAG)** system using:
- **Language Model**: Phi-3.1-mini-4k-instruct-GGUF (3.8B parameters, quantized Q4_K_M)
- **Retrieval**: FAISS semantic search with embeddings
- **Inference**: Streaming generation with real-time token output
- **Interface**: Interactive command-line chatbot with multi-turn conversations

---

## âœ… Completed Components

### 1. **src/rag_core/generator.py** âœ“
**Purpose**: Core LLM inference engine with Phi-3.1

**Key Features**:
- âœ… Auto-download Phi-3.1 model from Hugging Face Hub
- âœ… DeviceManager for GPU/CPU orchestration
- âœ… Automatic GPU VRAM detection and layer optimization
- âœ… Phi-3 chat format support with special tokens
- âœ… Streaming and non-streaming generation modes
- âœ… Full test coverage (Test 1-3 pass)

**Model Details**:
- Model Repo: `lmstudio-community/Phi-3.1-mini-4k-instruct-GGUF`
- Quantization: `*Q4_K_M.gguf` (~2.39GB)
- Context: 4096 tokens (default 2048 for RAG)
- GPU Strategy:
  - 6GB+: All 32 layers
  - 4GB: 32 layers
  - 3GB: 24 layers
  - <2GB: CPU fallback

---

### 2. **src/rag_core/retriever.py** âœ“
**Purpose**: Semantic search and document retrieval

**Key Features**:
- âœ… FAISS index for fast similarity search
- âœ… Sentence-transformers embeddings
- âœ… Hybrid ranking (vector + heading similarity)
- âœ… Top-k retrieval with configurable result count
- âœ… Chunk metadata handling with pickle support


---

### 3. **src/rag_core/rag_chain.py** âœ“
**Purpose**: Integration of Retriever + Generator into unified pipeline

**Key Features**:
- âœ… Combined retriever and generator initialization
- âœ… Automatic context formatting from retrieved documents
- âœ… Built-in document QA system prompt
- âœ… Streaming and non-streaming generation
- âœ… Interactive mode with REPL loop
- âœ… Full test coverage with streaming responses

**Workflow**:
1. User query comes in
2. Retriever.search(query) â†’ Find top-k relevant chunks
3. Format chunks as context: "[Document Passage 1]:\n{content}\n[Document Passage 2]:\n{content}..."
4. Build messages: [system_prompt, context_and_query]
5. Generator.generate(messages, stream=True/False) â†’ Response

---

### 4. **src/app/main.py** âœ“
**Purpose**: User-facing interactive chatbot application

**Key Features**:
- âœ… CLI argument parsing with argparse
- âœ… Interactive chat loop with streaming responses
- âœ… Conversation history tracking with timestamps
- âœ… Built-in commands: help, history, clear, quit, exit
- âœ… Real-time token streaming display
- âœ… Session summary on exit
- âœ… Comprehensive error handling and logging

---

## ðŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INPUT                              â”‚
â”‚                   (via CLI prompt)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 INTERACTIVE LOOP (main.py)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Parse: Commands (help, history, etc.) vs Query      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼ Query                           â–¼ Command
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RETRIEVER (Step 1)     â”‚    â”‚  Handle Command  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚ (display/clear)  â”‚
â”‚  â”‚ 1. Embed query     â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ 2. Search FAISS    â”‚  â”‚
â”‚  â”‚ 3. Top-k results   â”‚  â”‚
â”‚  â”‚ 4. Return chunks   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Context chunks
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORMAT CONTEXT (rag_chain.py)          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [Document Passage 1]:                â”‚ â”‚
â”‚ â”‚ {chunk content}                      â”‚ â”‚
â”‚ â”‚ [Document Passage 2]:                â”‚ â”‚
â”‚ â”‚ {chunk content}                      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BUILD MESSAGES (Phi-3 format)          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ {"role": "system",                   â”‚ â”‚
â”‚ â”‚  "content": "You are helpful..."}    â”‚ â”‚
â”‚ â”‚ {"role": "user",                     â”‚ â”‚
â”‚ â”‚  "content": "Context:\n...\nQ:..."}  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Formatted messages
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    GENERATOR (Step 2)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 1. Format to Phi-3 chat template     â”‚â”‚
â”‚  â”‚ 2. Load model if needed              â”‚â”‚
â”‚  â”‚ 3. Generate tokens                   â”‚â”‚
â”‚  â”‚ 4. Stream or collect response        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
        â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼ stream=True            â–¼ stream=False
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   STREAM    â”‚          â”‚  COLLECT ALL   â”‚
   â”‚  Yields     â”‚          â”‚  Return full   â”‚
   â”‚  tokens     â”‚          â”‚  string        â”‚
   â”‚  one-by-one â”‚          â”‚                â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Response text
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  DISPLAY (main.py)             â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
        â”‚ â”‚ Print tokens in real-time or â”‚â”‚
        â”‚ â”‚ display full response        â”‚â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SAVE TO HISTORY               â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
        â”‚ â”‚ {type: "user"/"assistant",   â”‚â”‚
        â”‚ â”‚  content: "...",             â”‚â”‚
        â”‚ â”‚  timestamp: ...}             â”‚â”‚
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  BACK TO USER PROMPT           â”‚
        â”‚  Ready for next query          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ File Structure

```
/home/thienta/HUST_20235839/AI/rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ main.py ................... âœ“ Interactive chatbot application
â”‚   â””â”€â”€ rag_core/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ generator.py .............. âœ“ Phi-3.1 LLM inference
â”‚       â”œâ”€â”€ retriever.py .............. âœ“ FAISS semantic search
â”‚       â””â”€â”€ rag_chain.py .............. âœ“ RAG pipeline integration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”œâ”€â”€ ocr/ .......................... OCR extracted documents
â”‚   â”œâ”€â”€ processed/ .................... Processed text
â”‚   â”œâ”€â”€ raw/ .......................... Raw documents
â”‚   â””â”€â”€ splitted/
â”‚       â””â”€â”€ chunks.jsonl .............. Document chunks
â”œâ”€â”€ requirements.txt .................. Python dependencies
â””â”€â”€ README.md (this file)

```

---

## ðŸ“ˆ Performance Metrics

### Model Size
- **Phi-3.1**: 3.8B parameters
- **Q4_K_M Quantization**: ~2.39GB disk
- **Context**: 4096 tokens (2048 default for RAG)

### GPU Optimization
- **Target**: 4GB VRAM
- **Strategy**: All 32 layers offloaded to GPU
- **Fallback**: CPU inference if GPU unavailable

### Inference Speed
- **First token**: ~2-3 seconds (loading + compute)
- **Subsequent tokens**: ~100-200ms per token on 4GB GPU
- **Streaming**: Real-time display of token generation

### Memory Usage
- **Model**: ~2.39GB on GPU
- **Embeddings**: Loaded on demand
- **Working Memory**: ~500MB-1GB (varies with context size)

---

## ðŸŽ“ Key Technologies

### Phi-3.1 LLM
- Small efficient language model (3.8B params)
- Optimized for 4K context length
- Quantized for low VRAM (Q4_K_M)
- Chat format support with special tokens

### FAISS (Facebook AI Similarity Search)
- Fast similarity search in high dimensions
- CPU and GPU support
- Efficient for millions of embeddings
- Used for finding relevant document chunks

### Sentence Transformers
- Semantic embeddings from text
- Pre-trained on sentence pairs
- Captures semantic meaning
- Used for query and document embeddings

### llama-cpp-python
- Python bindings for llama.cpp
- CPU optimized inference
- GPU support (CUDA, Metal)
- Streaming token generation
---

## ðŸ“ Summary

This RAG system provides:

âœ… **Efficient LLM**: Phi-3.1 optimized for 4GB GPU  
âœ… **Fast Retrieval**: FAISS semantic search  
âœ… **Streaming**: Real-time token generation  
âœ… **Interactive**: Multi-turn conversation loop  
âœ… **User-Friendly**: CLI with help and history  
âœ… **Well-Documented**: Code and usage guides  
âœ… **Production-Ready**: Error handling and logging  
