

Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstests2014 tests at a fraction of the training cost.   

<table><tr><td rowspan="2">Model</td><td colspan="2">BLEU</td><td colspan="2">Training Cost (FLOPs)</td></tr><tr><td>EN-DE</td><td>EN-FR</td><td>EN-DE</td><td>EN-FR</td></tr><tr><td>ByteNet [3]</td><td>23.75</td><td></td><td></td><td></td></tr><tr><td>Deep-Att + PosUnk [32]</td><td>39.2</td><td></td><td>1.0</td><td>1.0<sup>10</sup></td></tr><tr><td>GMNT + RL [21]</td><td>24.6</td><td>39.92</td><td>2.3 · 10<sup>19</sup></td><td>1.4 · 10<sup>20</sup></td></tr><tr><td>ConvSSS [8]</td><td>25.16</td><td>40.46</td><td>9.6 · 10<sup>18</sup></td><td>1.5 · 10<sup>20</sup></td></tr><tr><td>MoE [28]</td><td>26.03</td><td>40.56</td><td>2.0 · 10<sup>19</sup></td><td>1.2 · 10<sup>20</sup></td></tr><tr><td>Deep-Att + PosUnk Ensemble [32]</td><td></td><td>40.4</td><td></td><td>8.0 · 10<sup>20</sup></td></tr><tr><td>GMNT + RL Ensemble [21]</td><td>26.30</td><td>41.16</td><td>1.8 · 10<sup>20</sup></td><td>1.1 · 10<sup>21</sup></td></tr><tr><td>ConvSSS Ensemble [8]</td><td>26.36</td><td>41.29</td><td>7.7 · 10<sup>19</sup></td><td>1.2 · 10<sub>21</sub></td></tr><tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td>3.3 · 10<sup>18</sup></td><td></td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.0</td><td>2.3 · 10<sup>19</sup></td><td></td></tr></table>  


Label Smoothing During training, we employed label smoothing of value \(\epsilon_{l_{2}} = 0.1\) [29]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  


## 6 Results  


### 6.1 Machine Translation  


On the WMT 2014 English- to- German translation task, the big transformer model (Transformer (big) in Table 3) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state- of- the- art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 5 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.  


On the WMT 2014 English- to- French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than \(1 / 4\) the training cost of the previous state- of- the- art model. The Transformer (big) model trained for English- to- French used dropout rate \(P_{drop} = 0.1\) , instead of 0.3.  


For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10- minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \(\alpha = 0.6\) [32]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 30, but terminate early when possible [21].  


Table3 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single- precision floating- point capacity of each GPU.  


### 6.2 Model Variations  


To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English- to- German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3  


In Table5 [rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section [3.7.2]. While single- head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.