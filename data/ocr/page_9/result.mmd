
Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.  



<table><tr><td></td><td>N</td><td>dmodel</td><td>dgt</td><td>h</td><td>dks</td><td>dvs</td><td>Pdevop</td><td>ctrs</td><td>train steps</td><td>PPL (dev)</td><td>BLEU (dev)</td><td>params Ã—106</td></tr><tr><td rowspan="4">base</td><td rowspan="4">6</td><td rowspan="4">512</td><td rowspan="4">2048</td><td rowspan="4">8</td><td rowspan="4">64</td><td rowspan="4">64</td><td rowspan="4">0.1</td><td rowspan="4">0.1</td><td rowspan="4">100K</td><td rowspan="4">4.92</td><td rowspan="4">25.8</td><td rowspan="4">65</td></tr><tr></tr><tr></tr><tr></tr><tr><td rowspan="4">(A)</td><td rowspan="4"></td><td rowspan="4"></td><td rowspan="4"></td><td rowspan="2">1</td><td rowspan="2">512</td><td rowspan="2">512</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="4">5.20</td><td rowspan="4">24.9</td><td rowspan="4">25.5</td></tr><tr></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="4">(B)</td><td rowspan="4"></td><td rowspan="4"></td><td rowspan="-</td><td rowspan="4"></td><td rowspan="4"></td><td rowspan="3"></td><td rowspan="3"></td><td rowspan="3"></td><td rowspan="" rowspan="3"></td><td rowspan="3">5.01</td><td rowspan="3">25.4</td><td rowspan="3">58</td></tr><tr></tr><tr></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>  


In Table [rows (B), we observe that reducing the attention key size \(d_{k}\) hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function that do product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over- fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.  


## 7 Conclusion  


In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder- decoder architectures with multi- headed self- attention.  


For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English- to- German and WMT 2014 English- to- French translation tasks, we achieve a new state of the art. In the former task, our best model outperforms even all previously reported ensembles.  


We are excited about the future of attention- based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.  


The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor2  


Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.