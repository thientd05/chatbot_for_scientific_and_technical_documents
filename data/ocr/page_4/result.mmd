![](images/0.jpg)


<center>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. </center>  


query with all keys, divide each by \(\sqrt{d_{k}}\) , and apply a softmax function to obtain the weights on the values.  


In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix \(Q\) . The keys and values are also packed together into matrices \(K\) and \(V\) . We compute the matrix of outputs as:  


\[\mathrm{Attention}(Q,K,V) = \mathrm{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V \quad (1)\]  


The two most commonly used attention functions are additive attention [2], and dot- product (multi- plicative) attention. Dot- product attention is identical to our algorithm, except for the scaling factor of \(\frac{1}{\sqrt{d_{k}}}\) . Additive attention computes the compatibility function using a feed- forward network with a single hidden layer. While the two are similar in geometrical complexity, dot- product attention is much faster and more space- efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  


While for small values of \(d_{k}\) the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of \(d_{k}\) [3]. We suspect that for large values of \(d_{k}\) , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.1 To counteract this effect, we scale the dot products by \(\frac{1}{\sqrt{d_{k}}}\) .  


#### 3.2.2 Multi-Head Attention  


Instead of performing a single attention function with \(d_{\mathrm{head}}\) - dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values \(h\) times with different, learned linear projections to \(d_{k}\) , \(d_{k}\) , and \(d_{k}\) dimensions, respectively. On each of these projected versions of queries, keys and values are then perform the attention function in parallel. \(d_{k}\) - dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2  


Multi- head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.