"""
Retriever class cho RAG pipeline
- Load FAISS index v√† metadata
- Ph√¢n lo·∫°i query ƒë·ªÉ l·∫•y metadata ph√π h·ª£p
- Search vector trong t·ª´ng category
"""

import os
import json
import pickle
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import numpy as np

from sentence_transformers import SentenceTransformer
import faiss


@dataclass
class ChunkMetadata:
    """L∆∞u tr·ªØ metadata cho m·ªói chunk (ƒë·ªãnh nghƒ©a l·∫°i ·ªü ƒë√¢y ƒë·ªÉ d√πng chung)"""
    chunk_id: int  # ID c·ªßa chunk
    content: str  # N·ªôi dung chunk
    heading: Optional[str]  # Ti√™u ƒë·ªÅ cha


class Retriever:
    """
    Retriever class ƒë·ªÉ t√¨m ki·∫øm chunks d·ª±a tr√™n semantic search
    v·ªõi l·ªçc metadata d·ª±a tr√™n query classification
    
    Attributes:
        embeddings_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a embeddings
        model_name: T√™n model SentenceTransformer
        model: SentenceTransformer model
        index: FAISS index
        metadata_list: Danh s√°ch metadata
        heading_to_indices: Map t·ª´ heading ƒë·∫øn indices trong FAISS
    """
    
    def __init__(self, embeddings_dir: str = None):
        """
        Kh·ªüi t·∫°o Retriever
        
        Args:
            embeddings_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a embeddings
                           (m·∫∑c ƒë·ªãnh: ../../data/embeddings t·ª´ v·ªã tr√≠ file n√†y)
        """
        if embeddings_dir is None:
            # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh t·ª´ v·ªã tr√≠ file n√†y
            current_dir = os.path.dirname(os.path.abspath(__file__))
            embeddings_dir = os.path.join(current_dir, '../../data/embeddings')
        
        embeddings_dir = os.path.abspath(embeddings_dir)
        
        if not os.path.exists(embeddings_dir):
            raise FileNotFoundError(f"Embeddings directory not found: {embeddings_dir}")
        
        print(f"üìÇ Loading embeddings from: {embeddings_dir}")
        
        # Load config
        config_path = os.path.join(embeddings_dir, 'config.json')
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        self.model_name = config['model_name']
        self.embedding_dim = config['embedding_dim']
        
        print(f"üì• Loading model: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        
        # Load FAISS index
        index_path = os.path.join(embeddings_dir, 'faiss_index.bin')
        self.index = faiss.read_index(index_path)
        print(f"‚úÖ Loaded FAISS index (ntotal={self.index.ntotal})")
        
        # Load metadata
        metadata_path = os.path.join(embeddings_dir, 'metadata.pkl')
        with open(metadata_path, 'rb') as f:
            self.metadata_list = pickle.load(f)
        print(f"‚úÖ Loaded {len(self.metadata_list)} metadata entries")
        
        # Cache heading embeddings ƒë·ªÉ t·ªëi ∆∞u
        self.heading_embeddings_cache = {}
        
        # T·∫°o mapping t·ª´ heading ƒë·∫øn indices
        self._build_heading_index()
    
    def _build_heading_index(self):
        """T·∫°o mapping t·ª´ heading ƒë·∫øn danh s√°ch indices"""
        self.heading_to_indices = {}
        
        for idx, metadata in enumerate(self.metadata_list):
            heading = metadata.heading or 'N/A'
            if heading == "N/A" or heading == "References  ":
                continue
            if heading not in self.heading_to_indices:
                self.heading_to_indices[heading] = []
            self.heading_to_indices[heading].append(idx)
        
        print(f"‚úÖ Built heading index with {len(self.heading_to_indices)} unique headings")
    

    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        T√¨m ki·∫øm chunks d·ª±a tr√™n query
        
        C√¥ng th·ª©c t√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng:
        hybrid_score = 0.6 * t∆∞∆°ng_ƒë·ªìng_cosin(query_embedding, chunk_embedding) 
                     + 0.4 * t∆∞∆°ng_ƒë·ªìng_cosin(query_embedding, heading_embedding)
        
        Args:
            query: Query text
            top_k: S·ªë chunks c·∫ßn l·∫•y
        
        Returns:
            List[Dict]: Danh s√°ch k·∫øt qu·∫£ s·∫Øp x·∫øp theo hybrid_score gi·∫£m d·∫ßn
                Keys: chunk_id, content, heading, vector_similarity, heading_similarity, hybrid_score
        """
        print(f"\nüîç Query: '{query}'")
        
        # Embed query
        query_embedding = self.model.encode([query], show_progress_bar=False)
        query_embedding = np.array(query_embedding, dtype=np.float32)
        
        # Search tr√™n to√†n b·ªô index
        distances, indices = self.index.search(query_embedding, top_k)
        
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx < len(self.metadata_list):
                metadata = self.metadata_list[idx]
                
                # Convert L2 distance to cosine similarity
                vector_similarity = max(0.0, 1.0 - (distance ** 2) / 2.0)
                
                # T√≠nh heading similarity
                heading_similarity = 0.0
                if metadata.heading:
                    if metadata.heading not in self.heading_embeddings_cache:
                        heading_vec = self.model.encode([metadata.heading], show_progress_bar=False)
                        self.heading_embeddings_cache[metadata.heading] = np.array(heading_vec[0], dtype=np.float32)
                    
                    heading_embedding = self.heading_embeddings_cache[metadata.heading]
                    
                    norm_query = np.linalg.norm(query_embedding[0])
                    norm_heading = np.linalg.norm(heading_embedding)
                    
                    if norm_query > 0 and norm_heading > 0:
                        heading_similarity = float(np.dot(query_embedding[0], heading_embedding) / (norm_query * norm_heading))
                
                # Hybrid score: 0.6 * vector_sim + 0.4 * heading_sim
                hybrid_score = 0.6 * vector_similarity + 0.4 * heading_similarity
                
                results.append({
                    'chunk_id': metadata.chunk_id,
                    'content': metadata.content,
                    'heading': metadata.heading,
                    'vector_similarity': float(vector_similarity),
                    'heading_similarity': float(heading_similarity),
                    'hybrid_score': float(hybrid_score)
                })
        
        # Sort results by hybrid_score descending
        results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        return results


def test_retriever():
    """Test Retriever class"""
    print("=" * 80)
    print("TEST: Retriever")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o retriever
    retriever = Retriever()
    
    # Test queries
    test_queries = [
        "explain positional encoding",
    ]
    
    print("\n" + "=" * 80)
    print("TEST: Search Queries")
    print("=" * 80)
    
    for query in test_queries:
        results = retriever.search(query, top_k=3)
        
        # Print results
        print("-" * 80)
        for i, result in enumerate(results, 1):
            print(f"\n  [{i}] Chunk ID {result['chunk_id']}")
            print(f"      Heading: {result['heading'] or 'N/A'}")
            print(f"      Vector Similarity: {result['vector_similarity']:.4f}")
            print(f"      Heading Similarity: {result['heading_similarity']:.4f}")
            print(f"      Hybrid Score: {result['hybrid_score']:.4f}")
            content_display = result['content'][:100]
            if len(result['content']) > 100:
                content_display += "..."
            print(f"      Content: {content_display}")
        print()


if __name__ == "__main__":
    test_retriever()
