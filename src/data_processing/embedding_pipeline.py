"""
Embedding pipeline sá»­ dá»¥ng BAAI/bge-large-en-v1.5 vÃ  FAISS
"""

import os
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import pickle

from sentence_transformers import SentenceTransformer
import faiss


@dataclass
class ChunkMetadata:
    """LÆ°u trá»¯ metadata cho má»—i chunk"""
    chunk_id: int  # ID cá»§a chunk
    content: str  # Ná»™i dung chunk
    heading: Optional[str]  # TiÃªu Ä‘á» cha


class EmbeddingPipeline:
    """
    Pipeline Ä‘á»ƒ embedding text chunks vÃ  lÆ°u vÃ o FAISS
    
    Attributes:
        model_name: TÃªn model sá»­ dá»¥ng (máº·c Ä‘á»‹nh: BAAI/bge-large-en-v1.5)
        embedding_dim: Dimension cá»§a embedding (1024 cho BGE large)
        model: SentenceTransformer model
        index: FAISS index
        metadata_list: Danh sÃ¡ch metadata tÆ°Æ¡ng á»©ng vá»›i vectors
    """
    
    def __init__(self, model_name: str = "BAAI/bge-large-en-v1.5"):
        """
        Khá»Ÿi táº¡o EmbeddingPipeline
        
        Args:
            model_name: TÃªn model tá»« HuggingFace (máº·c Ä‘á»‹nh: BAAI/bge-large-en-v1.5)
        """
        self.model_name = model_name
        self.embedding_dim = 1024  # BGE large cÃ³ 1024 dimensions
        
        print(f"ğŸ“¥ Loading model: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        # FAISS index sá»­ dá»¥ng L2 distance
        self.index = faiss.IndexFlatL2(self.embedding_dim)
        
        # LÆ°u metadata tÆ°Æ¡ng á»©ng vá»›i má»—i vector
        self.metadata_list: List[ChunkMetadata] = []
        
        print(f"âœ… Model loaded. Embedding dimension: {self.embedding_dim}")
    
    def embed_chunks(self, chunks: List[Dict]) -> np.ndarray:
        """
        Embed danh sÃ¡ch chunks
        
        Args:
            chunks: Danh sÃ¡ch dict cÃ³ keys 'content' vÃ  'metadata'
        
        Returns:
            np.ndarray: Ma tráº­n embedding (n_chunks, embedding_dim)
        """
        print(f"\nğŸ“Š Embedding {len(chunks)} chunks...")
        
        # TrÃ­ch xuáº¥t content tá»« chunks
        contents = [chunk['content'] for chunk in chunks]
        
        # Embed sá»­ dá»¥ng model
        embeddings = self.model.encode(contents, show_progress_bar=True)
        
        # Chuyá»ƒn thÃ nh float32 cho FAISS
        embeddings = np.array(embeddings, dtype=np.float32)
        
        print(f"âœ… Embedding completed. Shape: {embeddings.shape}")
        
        return embeddings
    
    def add_chunks(self, chunks: List[Dict]) -> None:
        """
        ThÃªm chunks vÃ o FAISS index
        
        Args:
            chunks: Danh sÃ¡ch dict cÃ³ keys 'content' vÃ  'metadata'
        """
        # Embed chunks
        embeddings = self.embed_chunks(chunks)
        
        # ThÃªm vÃ o FAISS index
        self.index.add(embeddings)
        
        # LÆ°u metadata
        for i, chunk in enumerate(chunks):
            metadata = ChunkMetadata(
                chunk_id=len(self.metadata_list) + i,
                content=chunk['content'],
                heading=chunk['metadata'].get('heading')
            )
            self.metadata_list.append(metadata)
        
        print(f"âœ… Added {len(chunks)} chunks to index")
        print(f"   Total chunks in index: {len(self.metadata_list)}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        TÃ¬m kiáº¿m similar chunks cho query
        
        Args:
            query: Text query
            top_k: Sá»‘ chunks cáº§n láº¥y
        
        Returns:
            List[Dict]: Danh sÃ¡ch káº¿t quáº£ vá»›i keys:
                - chunk_id: ID cá»§a chunk
                - content: Ná»™i dung chunk
                - heading: TiÃªu Ä‘á» cha
                - distance: L2 distance tá»« query
                - similarity: Cosine similarity (0-1, higher is better)
        """
        # Embed query
        query_embedding = self.model.encode([query], show_progress_bar=False)
        query_embedding = np.array(query_embedding, dtype=np.float32)
        
        # Search in FAISS
        distances, indices = self.index.search(query_embedding, top_k)
        
        # Láº¥y metadata tá»« indices
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx < len(self.metadata_list):
                metadata = self.metadata_list[idx]
                
                # TÃ­nh cosine similarity tá»« L2 distance
                # L2_distance = sqrt(sum((a-b)^2))
                # cosine_similarity = 1 - L2_distance^2 / (2 * dim)
                # Hoáº·c sá»­ dá»¥ng cÃ´ng thá»©c: similarity = 1 / (1 + distance)
                similarity = 1.0 / (1.0 + distance)
                
                results.append({
                    'chunk_id': metadata.chunk_id,
                    'content': metadata.content,
                    'heading': metadata.heading,
                    'distance': float(distance),
                    'similarity': float(similarity)
                })
        
        return results
    
    def save(self, save_dir: str) -> None:
        """
        LÆ°u FAISS index vÃ  metadata
        
        Args:
            save_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c Ä‘á»ƒ lÆ°u
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # LÆ°u FAISS index
        index_path = os.path.join(save_dir, 'faiss_index.bin')
        faiss.write_index(self.index, index_path)
        print(f"âœ… Saved FAISS index to: {index_path}")
        
        # LÆ°u metadata
        metadata_path = os.path.join(save_dir, 'metadata.pkl')
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.metadata_list, f)
        print(f"âœ… Saved metadata to: {metadata_path}")
        
        # LÆ°u metadata dÆ°á»›i dáº¡ng JSON Ä‘á»ƒ dá»… Ä‘á»c
        metadata_json_path = os.path.join(save_dir, 'metadata.json')
        metadata_json = [asdict(m) for m in self.metadata_list]
        with open(metadata_json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata_json, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved metadata (JSON) to: {metadata_json_path}")
        
        # LÆ°u config
        config = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'num_chunks': len(self.metadata_list)
        }
        config_path = os.path.join(save_dir, 'config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)
        print(f"âœ… Saved config to: {config_path}")
    
    def load(self, save_dir: str) -> None:
        """
        Load FAISS index vÃ  metadata
        
        Args:
            save_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c Ä‘á»ƒ load
        """
        # Load FAISS index
        index_path = os.path.join(save_dir, 'faiss_index.bin')
        self.index = faiss.read_index(index_path)
        print(f"âœ… Loaded FAISS index from: {index_path}")
        
        # Load metadata
        metadata_path = os.path.join(save_dir, 'metadata.pkl')
        with open(metadata_path, 'rb') as f:
            self.metadata_list = pickle.load(f)
        print(f"âœ… Loaded metadata from: {metadata_path}")
        print(f"   Total chunks: {len(self.metadata_list)}")
    
    def get_statistics(self) -> Dict:
        """
        Láº¥y thá»‘ng kÃª vá» index
        
        Returns:
            Dict: Thá»‘ng kÃª bao gá»“m:
                - num_vectors: Sá»‘ vectors trong index
                - embedding_dim: Dimension cá»§a má»—i vector
                - num_chunks: Sá»‘ chunks
                - headings: Dict Ä‘áº¿m chunks theo heading
        """
        # Äáº¿m chunks theo heading
        heading_counts = {}
        for metadata in self.metadata_list:
            heading = metadata.heading or 'N/A'
            heading_counts[heading] = heading_counts.get(heading, 0) + 1
        
        return {
            'num_vectors': self.index.ntotal,
            'embedding_dim': self.embedding_dim,
            'num_chunks': len(self.metadata_list),
            'headings': heading_counts
        }
    
    def print_statistics(self) -> None:
        """In thá»‘ng kÃª index"""
        stats = self.get_statistics()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š EMBEDDING INDEX STATISTICS")
        print("=" * 80)
        print(f"Number of vectors: {stats['num_vectors']}")
        print(f"Embedding dimension: {stats['embedding_dim']}")
        print(f"Number of chunks: {stats['num_chunks']}")
        print(f"\nğŸ“‹ Chunks by heading (top 10):")
        
        for heading, count in sorted(stats['headings'].items(), 
                                     key=lambda x: x[1], 
                                     reverse=True)[:10]:
            heading_display = heading[:60] + '...' if len(heading) > 60 else heading
            print(f"  '{heading_display}': {count} chunks")
        
        print("=" * 80 + "\n")


def load_chunks_from_jsonl(jsonl_path: str) -> List[Dict]:
    """
    Load chunks tá»« JSONL file
    
    Args:
        jsonl_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file JSONL
    
    Returns:
        List[Dict]: Danh sÃ¡ch chunks
    """
    chunks = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                chunk = json.loads(line)
                chunks.append(chunk)
    
    print(f"âœ… Loaded {len(chunks)} chunks from {jsonl_path}")
    return chunks


def process_and_embed(
    chunks_jsonl_path: str,
    output_dir: str,
    model_name: str = "BAAI/bge-large-en-v1.5"
) -> EmbeddingPipeline:
    """
    HÃ m tiá»‡n lá»£i: Load chunks tá»« JSONL vÃ  táº¡o embedding index
    
    Args:
        chunks_jsonl_path: ÄÆ°á»ng dáº«n Ä‘áº¿n chunks.jsonl
        output_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c output
        model_name: TÃªn model
    
    Returns:
        EmbeddingPipeline: Pipeline Ä‘Ã£ embedding
    """
    # Load chunks
    chunks = load_chunks_from_jsonl(chunks_jsonl_path)
    
    # Táº¡o pipeline
    pipeline = EmbeddingPipeline(model_name=model_name)
    
    # ThÃªm chunks vÃ o index
    pipeline.add_chunks(chunks)
    
    # In thá»‘ng kÃª
    pipeline.print_statistics()
    
    # LÆ°u
    pipeline.save(output_dir)
    
    return pipeline
