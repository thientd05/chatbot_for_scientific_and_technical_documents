"""
Test script cho MarkdownChunker
"""

import sys
import os
from pathlib import Path
from text_splitter import MarkdownChunker, chunk_markdown_file
import json


def test_chunker_with_file():
    """Test chunker vá»›i file final_text.txt"""
    print("=" * 80)
    print("TEST: Chunking file final_text.txt")
    print("=" * 80)
    
    # ÄÆ°á»ng dáº«n file tuyá»‡t Ä‘á»‘i
    file_path = '/home/thienta/HUST_20235839/AI/rag/data/processed/final_text.txt'
    
    if not os.path.exists(file_path):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {file_path}")
        return []
    
    # Chunking
    print(f"ğŸ“‚ Äá»c file: {file_path}")
    chunks = chunk_markdown_file(file_path)
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ chunks: {len(chunks)}")
    
    # In ra 10 chunks Ä‘áº§u tiÃªn
    print(f"\nğŸ“ Chi tiáº¿t 10 chunks Ä‘áº§u tiÃªn:")
    print("-" * 80)
    
    for i, chunk in enumerate(chunks[:10], 1):
        print(f"\n[Chunk {i}]")
        print(f"  Heading: {chunk.metadata.get('heading', 'N/A')}")
        print(f"  Content: {chunk.content[:100]}{'...' if len(chunk.content) > 100 else ''}")
        print(f"  Length: {len(chunk.content)} kÃ½ tá»±")
    
    print("\n" + "=" * 80)
    
    # Thá»‘ng kÃª
    if chunks:
        print("\nğŸ“ˆ Thá»‘ng kÃª:")
        print("-" * 80)
        
        # Äáº¿m chunks theo heading
        heading_counts = {}
        for chunk in chunks:
            heading = chunk.metadata.get('heading', 'No heading')
            heading_counts[heading] = heading_counts.get(heading, 0) + 1
        
        print("\nğŸ“‹ Sá»‘ chunks theo heading (top 10):")
        for heading, count in sorted(heading_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            heading_display = heading[:60] + '...' if heading and len(heading) > 60 else (heading or 'N/A')
            print(f"  '{heading_display}': {count} chunks")
        
        # Thá»‘ng kÃª Ä‘á»™ dÃ i
        lengths = [len(chunk.content) for chunk in chunks]
        print(f"\nğŸ“Š Äá»™ dÃ i chunks:")
        print(f"  Min: {min(lengths)} kÃ½ tá»±")
        print(f"  Max: {max(lengths)} kÃ½ tá»±")
        print(f"  Trung bÃ¬nh: {sum(lengths) // len(lengths):.0f} kÃ½ tá»±")
    
    print("\n" + "=" * 80)
    
    return chunks

def test_save_to_jsonl(chunks):
    """Test lÆ°u chunks thÃ nh file JSONL"""
    print("\n" + "=" * 80)
    print("TEST: LÆ°u chunks thÃ nh file JSONL")
    print("=" * 80)
    
    if not chunks:
        print("âŒ KhÃ´ng cÃ³ chunks Ä‘á»ƒ lÆ°u")
        return
    
    # LÆ°u thÃ nh JSONL
    output_path = '/home/thienta/HUST_20235839/AI/rag/data/processed/chunks.jsonl'
    
    chunker = MarkdownChunker()
    chunker.save_to_jsonl(chunks, output_path)
    
    print(f"\nâœ… ÄÃ£ lÆ°u {len(chunks)} chunks thÃ nh file: {output_path}")
    
    # In ra 5 dÃ²ng Ä‘áº§u tiÃªn cá»§a file JSONL
    print(f"\nğŸ“ 5 dÃ²ng Ä‘áº§u tiÃªn cá»§a file JSONL:")
    print("-" * 80)
    
    with open(output_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f, 1):
            if i <= 5:
                data = json.loads(line)
                print(f"\nLine {i}:")
                print(f"  Heading: {data['metadata'].get('heading', 'N/A')}")
                print(f"  Content: {data['content'][:80]}{'...' if len(data['content']) > 80 else ''}")
            else:
                break
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    chunks = test_chunker_with_file()
    
    # Test 3: LÆ°u thÃ nh JSONL
    test_save_to_jsonl(chunks)
    
    print("\nâœ… Táº¥t cáº£ tests hoÃ n táº¥t!")
